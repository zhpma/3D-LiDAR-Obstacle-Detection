{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install shapely -U\n",
    "!pip install lyft-dataset-sdk"
   ],
   "id": "d6c416057f23da31"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 定义用于存储生成的数据、可视化结果和模型检查点的文件夹\n",
    "ARTIFACTS_FOLDER = \"./artifacts\""
   ],
   "id": "956b01e19daef85a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from datetime import datetime\n",
    "from functools import partial\n",
    "import glob\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import scipy\n",
    "import scipy.ndimage\n",
    "import scipy.special\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "from lyft_dataset_sdk.lyftdataset import LyftDataset\n",
    "from lyft_dataset_sdk.utils.data_classes import LidarPointCloud, Box, Quaternion\n",
    "from lyft_dataset_sdk.utils.geometry_utils import view_points, transform_matrix"
   ],
   "id": "b502e447d703c2cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 创建符号链接，访问对应数据目录\n",
    "!ln -s /kaggle/input/3d-object-detection-for-autonomous-vehicles/train_images images\n",
    "!ln -s /kaggle/input/3d-object-detection-for-autonomous-vehicles/train_maps maps\n",
    "!ln -s /kaggle/input/3d-object-detection-for-autonomous-vehicles/train_lidar lidar"
   ],
   "id": "148b3c821a78e255"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 加载数据集，创建输出目录\n",
    "level5data = LyftDataset(data_path='.', json_path='/kaggle/input/3d-object-detection-for-autonomous-vehicles/train_data', verbose=True)\n",
    "os.makedirs(ARTIFACTS_FOLDER, exist_ok=True)"
   ],
   "id": "4de8caa0bddbc3d5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 数据集中可能出现的物体类别\n",
    "classes = [\"car\", \"motorcycle\", \"bus\", \"bicycle\", \"truck\", \"pedestrian\", \"other_vehicle\", \"animal\", \"emergency_vehicle\"]"
   ],
   "id": "a1ed5c5e41338f2c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 获取场景信息，提取每个场景的关键信息（如场景名称、时间戳、token 等），并将其存储到一个列表中\n",
    "records = [(level5data.get('sample', record['first_sample_token'])['timestamp'], record) for record in level5data.scene]\n",
    "\n",
    "entries = []\n",
    "\n",
    "for start_time, record in sorted(records):\n",
    "    start_time = level5data.get('sample', record['first_sample_token'])['timestamp'] / 1000000\n",
    "\n",
    "    token = record['token']\n",
    "    name = record['name']\n",
    "    date = datetime.utcfromtimestamp(start_time)\n",
    "    host = \"-\".join(record['name'].split(\"-\")[:2])\n",
    "    first_sample_token = record[\"first_sample_token\"]\n",
    "\n",
    "    entries.append((host, name, date, token, first_sample_token))\n",
    "            \n",
    "df = pd.DataFrame(entries, columns=[\"host\", \"scene_name\", \"date\", \"scene_token\", \"first_sample_token\"])"
   ],
   "id": "a2b581eeaf6c44fb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 按 host不同车辆对数据进行分组\n",
    "host_count_df = df.groupby(\"host\")['scene_token'].count()\n",
    "print(host_count_df)"
   ],
   "id": "d40b6fca9a6d13c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 拆分为训练集和验证集，\n",
    "validation_hosts = [\"host-a007\", \"host-a008\", \"host-a009\"]#定义了验证集的车辆host\n",
    "\n",
    "validation_df = df[df[\"host\"].isin(validation_hosts)]\n",
    "vi = validation_df.index\n",
    "train_df = df[~df.index.isin(vi)]\n",
    "\n",
    "print(len(train_df), len(validation_df), \"train/validation split scene counts\")"
   ],
   "id": "6bfa75259ce38865"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 获取训练集中的第一个样本的 token\n",
    "sample_token = train_df.first_sample_token.values[0]\n",
    "\n",
    "# 使用 sample_token 从数据集中获取对应的样本数据\n",
    "sample = level5data.get(\"sample\", sample_token)\n",
    "\n",
    "# 获取当前样本中 LIDAR_TOP 数据的 token\n",
    "sample_lidar_token = sample[\"data\"][\"LIDAR_TOP\"]\n",
    "\n",
    "# 根据 lidar 数据的 token 获取对应的 LiDAR 数据\n",
    "lidar_data = level5data.get(\"sample_data\", sample_lidar_token)\n",
    "\n",
    "# 获取 LiDAR 数据的文件路径\n",
    "lidar_filepath = level5data.get_sample_data_path(sample_lidar_token)\n",
    "\n",
    "# 获取当前样本对应的 ego pose（车辆自身坐标系到世界坐标系的变换信息）\n",
    "ego_pose = level5data.get(\"ego_pose\", lidar_data[\"ego_pose_token\"])\n",
    "\n",
    "# 获取当前样本对应的校准传感器信息（从传感器坐标系到车辆坐标系的变换信息）\n",
    "calibrated_sensor = level5data.get(\"calibrated_sensor\", lidar_data[\"calibrated_sensor_token\"])\n",
    "\n",
    "# 计算从车辆坐标系到世界坐标系的变换矩阵（即车辆坐标系到世界坐标系的转换）\n",
    "global_from_car = transform_matrix(ego_pose['translation'],\n",
    "                                   Quaternion(ego_pose['rotation']), inverse=False)\n",
    "\n",
    "# 计算从传感器坐标系到车辆坐标系的变换矩阵（即传感器坐标系到车辆坐标系的转换）\n",
    "car_from_sensor = transform_matrix(calibrated_sensor['translation'], Quaternion(calibrated_sensor['rotation']),inverse=False)\n"
   ],
   "id": "92a8262710e9977f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 从文件中加载 LiDAR 点云数据\n",
    "lidar_pointcloud = LidarPointCloud.from_file(lidar_filepath)\n",
    "\n",
    "# 点云数据是以传感器坐标系为参考系定义的。\n",
    "# 现在我们希望将其转换到车辆坐标系，因此我们对每个点进行坐标转换\n",
    "lidar_pointcloud.transform(car_from_sensor)\n",
    "\n",
    "# 做一个合理性检查，确保点云数据应该在车辆坐标系中接近于 0（即车辆中心）\n",
    "plt.hist(lidar_pointcloud.points[0], alpha=0.5, bins=30, label=\"X\")  # 绘制 X 轴方向的点云分布\n",
    "plt.hist(lidar_pointcloud.points[1], alpha=0.5, bins=30, label=\"Y\")  # 绘制 Y 轴方向的点云分布\n",
    "plt.legend()  # 显示图例\n",
    "plt.xlabel(\"Distance from car along axis\")  # X 轴标签：距离车辆沿轴方向的距离\n",
    "plt.ylabel(\"Amount of points\")  # Y 轴标签：点的数量\n",
    "plt.show()  # 显示图像\n"
   ],
   "id": "54b03566dfbcdd69"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 点云-->体素\n",
    "def create_transformation_matrix_to_voxel_space(shape, voxel_size, offset):\n",
    "    \"\"\"\n",
    "    构建一个变换矩阵，将点云数据从世界坐标系转换到体素空间坐标系，使得体素空间的 (0,0,0) 位于中心。\n",
    "    voxel_size 定义了每个体素在世界坐标系中的大小，(1,1,1) 就像 Minecraft 中的体素。\n",
    "    \n",
    "    可以提供每个轴的偏移量（以世界坐标为单位），这在 Z 轴（上下）方向尤其对于 lidar 点云有用。\n",
    "    \"\"\"\n",
    "    \n",
    "    shape, voxel_size, offset = np.array(shape), np.array(voxel_size), np.array(offset)\n",
    "    \n",
    "    # 创建一个 4x4 的单位矩阵，用于变换\n",
    "    tm = np.eye(4, dtype=np.float32)\n",
    "    \n",
    "    # 计算平移量，使得体素空间的原点位于体素网格的中心\n",
    "    translation = shape / 2 + offset / voxel_size\n",
    "    \n",
    "    # 设置缩放（将每个体素的大小按 voxel_size 进行缩放）\n",
    "    tm = tm * np.array(np.hstack((1 / voxel_size, [1])))\n",
    "    \n",
    "    # 设置平移部分，使得体素空间的原点位于网格的中心\n",
    "    tm[:3, 3] = np.transpose(translation)\n",
    "    \n",
    "    return tm\n",
    "\n",
    "def transform_points(points, transf_matrix):\n",
    "    \"\"\"\n",
    "    使用变换矩阵将 (3,N) 或 (4,N) 形状的点云数据进行变换。\n",
    "    \"\"\"\n",
    "    if points.shape[0] not in [3, 4]:\n",
    "        raise Exception(\"输入的点云数据应该是 (3,N) 或 (4,N) 的形状，当前形状是 {}\".format(points.shape))\n",
    "    \n",
    "    # 使用变换矩阵对点云数据进行变换\n",
    "    return transf_matrix.dot(np.vstack((points[:3, :], np.ones(points.shape[1]))))[:3, :]\n",
    "\n",
    "# 使用一些示例值来测试函数\n",
    "# 创建一个变换矩阵，将点云转换到体素空间\n",
    "tm = create_transformation_matrix_to_voxel_space(shape=(100, 100, 4), voxel_size=(0.5, 0.5, 0.5), offset=(0, 0, 0.5))\n",
    "\n",
    "# 创建一个示例点云，格式为 (3, N)，其中 N 为点的数量\n",
    "p = transform_points(np.array([[10, 10, 0, 0, 0], [10, 5, 0, 0, 0], [0, 0, 0, 2, 0]], dtype=np.float32), tm)\n",
    "\n",
    "# 打印变换后的点云数据\n",
    "print(p)\n"
   ],
   "id": "f6cd56af624c2462"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 将 LiDAR 点云数据转换为体素网格，并进行可视化处理的过程。最终生成的是一个 BEV 图像（鸟瞰图）\n",
    "def car_to_voxel_coords(points, shape, voxel_size, z_offset=0):\n",
    "    # 检查输入的体素网格形状是否为三维\n",
    "    if len(shape) != 3:\n",
    "        raise Exception(\"Voxel volume shape should be 3 dimensions (x,y,z)\")\n",
    "        \n",
    "    # 检查点云数据的形状是否为 (3, N) 或 (4, N)，如果不符合则抛出异常\n",
    "    if len(points.shape) != 2 or points.shape[0] not in [3, 4]:\n",
    "        raise Exception(\"Input points should be (3,N) or (4,N) in shape, found {}\".format(points.shape))\n",
    "\n",
    "    # 创建从世界坐标系到体素坐标系的变换矩阵\n",
    "    tm = create_transformation_matrix_to_voxel_space(shape, voxel_size, (0, 0, z_offset))\n",
    "    \n",
    "    # 使用变换矩阵转换点云坐标\n",
    "    p = transform_points(points, tm)\n",
    "    return p\n",
    "\n",
    "\n",
    "def create_voxel_pointcloud(points, shape, voxel_size=(0.5, 0.5, 1), z_offset=0):\n",
    "    # 将点云坐标转换到体素坐标系\n",
    "    points_voxel_coords = car_to_voxel_coords(points.copy(), shape, voxel_size, z_offset)\n",
    "    \n",
    "    # 转换后的点云坐标，只取前三个维度\n",
    "    points_voxel_coords = points_voxel_coords[:3].transpose(1, 0)\n",
    "    \n",
    "    # 将坐标转换为整数类型\n",
    "    points_voxel_coords = np.int0(points_voxel_coords)\n",
    "    \n",
    "    # 创建一个空的体素网格，尺寸为给定的 shape\n",
    "    bev = np.zeros(shape, dtype=np.float32)\n",
    "    bev_shape = np.array(shape)\n",
    "\n",
    "    # 检查点云坐标是否在体素网格的有效范围内\n",
    "    within_bounds = (np.all(points_voxel_coords >= 0, axis=1) * np.all(points_voxel_coords < bev_shape, axis=1))\n",
    "    \n",
    "    # 保留有效的坐标\n",
    "    points_voxel_coords = points_voxel_coords[within_bounds]\n",
    "    \n",
    "    # 找到每个体素的位置和该位置的点云数量（即强度）\n",
    "    coord, count = np.unique(points_voxel_coords, axis=0, return_counts=True)\n",
    "        \n",
    "    # 注意：X 和 Y 被交换了，这里填充体素网格\n",
    "    bev[coord[:, 1], coord[:, 0], coord[:, 2]] = count\n",
    "    \n",
    "    return bev\n",
    "\n",
    "\n",
    "def normalize_voxel_intensities(bev, max_intensity=16):\n",
    "    # 将体素网格中的强度值归一化到 0 到 1 之间\n",
    "    return (bev / max_intensity).clip(0, 1)\n",
    "\n",
    "\n",
    "# 设置体素网格的尺寸和 Z 轴偏移量\n",
    "voxel_size = (0.4, 0.4, 1.5)\n",
    "z_offset = -2.0\n",
    "bev_shape = (336, 336, 3)\n",
    "\n",
    "# 创建体素点云（BEV 图像）\n",
    "bev = create_voxel_pointcloud(lidar_pointcloud.points, bev_shape, voxel_size=voxel_size, z_offset=z_offset)\n",
    "\n",
    "# 归一化体素强度值，使其范围为 [0, 1]\n",
    "bev = normalize_voxel_intensities(bev)\n"
   ],
   "id": "a426248cffc7b638"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T15:58:45.260500Z",
     "start_time": "2025-01-01T15:58:45.213007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 展示鸟瞰图\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.imshow(bev)\n",
    "plt.show()"
   ],
   "id": "28fc6a3e42d2c163",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mplt\u001B[49m\u001B[38;5;241m.\u001B[39mfigure(figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m16\u001B[39m,\u001B[38;5;241m8\u001B[39m))\n\u001B[0;32m      2\u001B[0m plt\u001B[38;5;241m.\u001B[39mimshow(bev)\n\u001B[0;32m      3\u001B[0m plt\u001B[38;5;241m.\u001B[39mshow()\n",
      "\u001B[1;31mNameError\u001B[0m: name 'plt' is not defined"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 将 LiDAR 数据中的物体框从世界坐标系转换到车辆坐标系，并将它们绘制在之前生成的鸟瞰图\n",
    "# 获取物体框（bounding boxes）数据\n",
    "boxes = level5data.get_boxes(sample_lidar_token)\n",
    "\n",
    "# 初始化目标图像，大小与之前生成的鸟瞰图（BEV）一致\n",
    "target_im = np.zeros(bev.shape[:3], dtype=np.uint8)\n",
    "\n",
    "def move_boxes_to_car_space(boxes, ego_pose):\n",
    "    \"\"\"\n",
    "    将物体框从世界坐标系转换到车辆坐标系。\n",
    "    注意：此函数会直接修改输入的物体框。\n",
    "    \"\"\"\n",
    "    translation = -np.array(ego_pose['translation'])  # 获取车辆位姿的平移部分\n",
    "    rotation = Quaternion(ego_pose['rotation']).inverse  # 获取车辆位姿的旋转部分，并取其逆\n",
    "\n",
    "    # 对每个物体框进行转换\n",
    "    for box in boxes:\n",
    "        box.translate(translation)  # 进行平移变换\n",
    "        box.rotate(rotation)  # 进行旋转变换\n",
    "\n",
    "def scale_boxes(boxes, factor):\n",
    "    \"\"\"\n",
    "    缩放物体框的尺寸。\n",
    "    注意：此函数会直接修改输入的物体框。\n",
    "    \"\"\"\n",
    "    for box in boxes:\n",
    "        box.wlh = box.wlh * factor  # 按照给定的比例因子缩放物体框的宽度、长度和高度\n",
    "\n",
    "def draw_boxes(im, voxel_size, boxes, classes, z_offset=0.0):\n",
    "    \"\"\"\n",
    "    将物体框绘制到图像上。\n",
    "    im: 目标图像，voxel_size: 体素大小，boxes: 物体框，classes: 类别，z_offset: Z轴偏移\n",
    "    \"\"\"\n",
    "    for box in boxes:\n",
    "        # 只关心物体框的底部四个角\n",
    "        corners = box.bottom_corners()  # 获取底部四个角的坐标\n",
    "        corners_voxel = car_to_voxel_coords(corners, im.shape, voxel_size, z_offset).transpose(1,0)  # 转换到体素坐标系\n",
    "        corners_voxel = corners_voxel[:,:2]  # 只保留 X 和 Y 坐标，丢弃 Z 坐标\n",
    "\n",
    "        # 根据类别为物体框分配颜色\n",
    "        class_color = classes.index(box.name) + 1  # 获取物体框的类别，并为其分配颜色\n",
    "        \n",
    "        # 如果类别未知，抛出异常\n",
    "        if class_color == 0:\n",
    "            raise Exception(\"Unknown class: {}\".format(box.name))\n",
    "\n",
    "        # 绘制物体框的轮廓，填充该区域\n",
    "        cv2.drawContours(im, np.int0([corners_voxel]), 0, (class_color, class_color, class_color), -1)\n",
    "\n",
    "# 将物体框从世界坐标系转换到车辆坐标系\n",
    "move_boxes_to_car_space(boxes, ego_pose)\n",
    "\n",
    "# 缩放物体框的尺寸\n",
    "scale_boxes(boxes, 0.8)\n",
    "\n",
    "# 将转换后的物体框绘制到目标图像上\n",
    "draw_boxes(target_im, voxel_size, boxes, classes, z_offset=z_offset)\n",
    "\n",
    "# 显示绘制后的图像，物体框用不同颜色表示不同类别\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow((target_im > 0).astype(np.float32), cmap='Set2')\n",
    "plt.show()\n"
   ],
   "id": "c2bf1d3df330b817"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 可视化点云\n",
    "def visualize_lidar_of_sample(sample_token, axes_limit=80):\n",
    "    sample = level5data.get(\"sample\", sample_token)\n",
    "    sample_lidar_token = sample[\"data\"][\"LIDAR_TOP\"]\n",
    "    level5data.render_sample_data(sample_lidar_token, axes_limit=axes_limit)\n",
    "    \n",
    "visualize_lidar_of_sample(sample_token)"
   ],
   "id": "226b05265c2101a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 删除之前的变量，以便释放内存\n",
    "del bev, lidar_pointcloud, boxes\n",
    "\n",
    "# 定义一些超参数，后续系统需要使用\n",
    "voxel_size = (0.4, 0.4, 1.5)  # 每个体素（Voxel）在三个维度上的大小，单位为米 (m)\n",
    "z_offset = -2.0  # LiDAR 数据在 Z 轴上的偏移量，通常用于调整 LiDAR 数据的垂直坐标\n",
    "bev_shape = (336, 336, 3)  # BEV（鸟瞰图）的尺寸，336x336 的图像，3个通道表示 RGB\n",
    "\n",
    "# 设置一个缩放因子，用于调整物体框大小，使其在鸟瞰图中更加分离\n",
    "box_scale = 0.8\n",
    "\n",
    "# 设置训练和验证数据的存储路径\n",
    "train_data_folder = os.path.join(ARTIFACTS_FOLDER, \"bev_train_data\")  # 训练数据文件夹路径\n",
    "validation_data_folder = os.path.join(ARTIFACTS_FOLDER, \"./bev_validation_data\")  # 验证数据文件夹路径\n"
   ],
   "id": "8170dd2c8601a7c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 定义一个常量，表示使用的工作进程数，通常是系统 CPU 核心数的三倍\n",
    "NUM_WORKERS = os.cpu_count() * 3\n",
    "\n",
    "def prepare_training_data_for_scene(first_sample_token, output_folder, bev_shape, voxel_size, z_offset, box_scale):\n",
    "    \"\"\"\n",
    "    给定场景的第一个样本 token，输出鸟瞰图视角下的栅格化输入体素数据和目标框数据。\n",
    "    \"\"\"\n",
    "    sample_token = first_sample_token\n",
    "    \n",
    "    while sample_token:\n",
    "        # 获取当前样本的详细信息\n",
    "        sample = level5data.get(\"sample\", sample_token)\n",
    "\n",
    "        # 获取与该样本相关的 LiDAR 数据 token\n",
    "        sample_lidar_token = sample[\"data\"][\"LIDAR_TOP\"]\n",
    "        lidar_data = level5data.get(\"sample_data\", sample_lidar_token)\n",
    "        lidar_filepath = level5data.get_sample_data_path(sample_lidar_token)\n",
    "\n",
    "        # 获取车辆位置（ego_pose）和校准传感器信息（calibrated_sensor）\n",
    "        ego_pose = level5data.get(\"ego_pose\", lidar_data[\"ego_pose_token\"])\n",
    "        calibrated_sensor = level5data.get(\"calibrated_sensor\", lidar_data[\"calibrated_sensor_token\"])\n",
    "\n",
    "        # 计算从世界坐标到车辆坐标的转换矩阵\n",
    "        global_from_car = transform_matrix(ego_pose['translation'],\n",
    "                                           Quaternion(ego_pose['rotation']), inverse=False)\n",
    "\n",
    "        # 计算从传感器坐标到车辆坐标的转换矩阵\n",
    "        car_from_sensor = transform_matrix(calibrated_sensor['translation'], Quaternion(calibrated_sensor['rotation']),\n",
    "                                            inverse=False)\n",
    "\n",
    "        try:\n",
    "            # 从文件中加载 LiDAR 点云数据，并将其从传感器坐标系转换到车辆坐标系\n",
    "            lidar_pointcloud = LidarPointCloud.from_file(lidar_filepath)\n",
    "            lidar_pointcloud.transform(car_from_sensor)\n",
    "        except Exception as e:\n",
    "            print(\"无法加载 LiDAR 点云数据 {}: {}\".format(sample_token, e))\n",
    "            sample_token = sample[\"next\"]  # 跳过当前样本，继续处理下一个样本\n",
    "            continue\n",
    "        \n",
    "        # 使用 LiDAR 点云生成鸟瞰图体素数据\n",
    "        bev = create_voxel_pointcloud(lidar_pointcloud.points, bev_shape, voxel_size=voxel_size, z_offset=z_offset)\n",
    "        # 对鸟瞰图体素强度进行归一化\n",
    "        bev = normalize_voxel_intensities(bev)\n",
    "\n",
    "        # 获取当前样本的目标框\n",
    "        boxes = level5data.get_boxes(sample_lidar_token)\n",
    "\n",
    "        # 创建一个与 BEV 相同形状的空白目标图像\n",
    "        target = np.zeros_like(bev)\n",
    "\n",
    "        # 将目标框从世界坐标系转换到车辆坐标系\n",
    "        move_boxes_to_car_space(boxes, ego_pose)\n",
    "        # 缩放目标框\n",
    "        scale_boxes(boxes, box_scale)\n",
    "        # 在目标图像上绘制目标框\n",
    "        draw_boxes(target, voxel_size, boxes=boxes, classes=classes, z_offset=z_offset)\n",
    "\n",
    "        # 将 BEV 图像的值放大到 255，并转换为 8 位无符号整型\n",
    "        bev_im = np.round(bev*255).astype(np.uint8)\n",
    "        # 选择目标图像的一个通道（通常是第一通道）\n",
    "        target_im = target[:,:,0]\n",
    "\n",
    "        # 保存输入的鸟瞰图和目标图像为 PNG 文件\n",
    "        cv2.imwrite(os.path.join(output_folder, \"{}_input.png\".format(sample_token)), bev_im)\n",
    "        cv2.imwrite(os.path.join(output_folder, \"{}_target.png\".format(sample_token)), target_im)\n",
    "        \n",
    "        # 获取下一个样本 token，继续处理下一个样本\n",
    "        sample_token = sample[\"next\"]\n",
    "\n",
    "# 处理训练集和验证集的每个样本\n",
    "for df, data_folder in [(train_df, train_data_folder), (validation_df, validation_data_folder)]:\n",
    "    print(\"使用 {} 个工作进程准备数据并保存到 {}\".format(NUM_WORKERS, data_folder))\n",
    "    # 获取第一个样本 token\n",
    "    first_samples = df.first_sample_token.values\n",
    "\n",
    "    # 确保输出文件夹存在\n",
    "    os.makedirs(data_folder, exist_ok=True)\n",
    "    \n",
    "    # 准备处理函数，使用部分函数（partial）将其它参数固定\n",
    "    process_func = partial(prepare_training_data_for_scene,\n",
    "                           output_folder=data_folder, bev_shape=bev_shape, voxel_size=voxel_size, z_offset=z_offset, box_scale=box_scale)\n",
    "\n",
    "    # 使用多进程池并行化处理任务\n",
    "    pool = Pool(NUM_WORKERS)\n",
    "    # 使用 tqdm_notebook 显示处理进度\n",
    "    for _ in tqdm_notebook(pool.imap_unordered(process_func, first_samples), total=len(first_samples)):\n",
    "        pass\n",
    "    pool.close()  # 关闭进程池\n",
    "    del pool  # 删除进程池对象，释放资源\n"
   ],
   "id": "91af95a6f066a657"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 定义BEV图像数据集类\n",
    "class BEVImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_filepaths, target_filepaths, map_filepaths=None):\n",
    "        \"\"\"\n",
    "        初始化BEV图像数据集\n",
    "\n",
    "        :param input_filepaths: 输入图像文件路径列表（BEV图像）\n",
    "        :param target_filepaths: 目标图像文件路径列表（目标框图像）\n",
    "        :param map_filepaths: 可选的地图文件路径列表\n",
    "        \"\"\"\n",
    "        self.input_filepaths = input_filepaths  # 存储输入图像路径\n",
    "        self.target_filepaths = target_filepaths  # 存储目标图像路径\n",
    "        self.map_filepaths = map_filepaths  # 存储地图文件路径（如果有）\n",
    "\n",
    "        # 如果提供了地图文件路径，确保输入和地图文件路径数量一致\n",
    "        if map_filepaths is not None:\n",
    "            assert len(input_filepaths) == len(map_filepaths)\n",
    "\n",
    "        # 确保输入图像路径和目标图像路径数量一致\n",
    "        assert len(input_filepaths) == len(target_filepaths)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        返回数据集的大小\n",
    "        \"\"\"\n",
    "        return len(self.input_filepaths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        根据索引返回图像和目标图像\n",
    "\n",
    "        :param idx: 索引\n",
    "        :return: 输入图像、目标图像和样本的token（用于标识样本）\n",
    "        \"\"\"\n",
    "        input_filepath = self.input_filepaths[idx]  # 获取输入图像路径\n",
    "        target_filepath = self.target_filepaths[idx]  # 获取目标图像路径\n",
    "        \n",
    "        # 从路径中提取样本token（通过去除\"_input.png\"得到样本ID）\n",
    "        sample_token = input_filepath.split(\"/\")[-1].replace(\"_input.png\", \"\")\n",
    "\n",
    "        # 读取输入图像\n",
    "        im = cv2.imread(input_filepath, cv2.IMREAD_UNCHANGED)\n",
    "        \n",
    "        # 如果有地图文件路径，将地图图像与输入图像拼接\n",
    "        if self.map_filepaths:\n",
    "            map_filepath = self.map_filepaths[idx]\n",
    "            map_im = cv2.imread(map_filepath, cv2.IMREAD_UNCHANGED)\n",
    "            im = np.concatenate((im, map_im), axis=2)  # 在通道维度拼接输入图像和地图图像\n",
    "        \n",
    "        # 读取目标图像\n",
    "        target = cv2.imread(target_filepath, cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "        # 将输入图像转为浮动型，并将值归一化到[0, 1]\n",
    "        im = im.astype(np.float32) / 255\n",
    "        \n",
    "        # 将目标图像转换为整数类型（标签）\n",
    "        target = target.astype(np.int64)\n",
    "        \n",
    "        # 将图像和目标从NumPy数组转换为PyTorch张量，并调整维度\n",
    "        im = torch.from_numpy(im.transpose(2, 0, 1))  # 转换维度为[C, H, W]\n",
    "        target = torch.from_numpy(target)\n",
    "\n",
    "        return im, target, sample_token  # 返回图像、目标和样本的token\n",
    "\n",
    "# 获取训练数据文件路径\n",
    "input_filepaths = sorted(glob.glob(os.path.join(train_data_folder, \"*_input.png\")))\n",
    "target_filepaths = sorted(glob.glob(os.path.join(train_data_folder, \"*_target.png\")))\n",
    "\n",
    "# 创建BEV图像数据集实例\n",
    "train_dataset = BEVImageDataset(input_filepaths, target_filepaths)\n",
    "\n",
    "# 获取数据集中的第二个样本（索引为1）\n",
    "im, target, sample_token = train_dataset[1]\n",
    "\n",
    "# 将图像和目标转为NumPy数组，以便进行处理\n",
    "im = im.numpy()\n",
    "target = target.numpy()\n",
    "\n",
    "# 创建图像窗口并显示图像\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "# 将目标图像复制为RGB格式，方便显示\n",
    "target_as_rgb = np.repeat(target[..., None], 3, 2)\n",
    "\n",
    "# 将输入图像的维度从[C, H, W]调整为[H, W, C]，符合Matplotlib的要求\n",
    "# 使用np.hstack将BEV图像和目标图像拼接在一起显示\n",
    "plt.imshow(np.hstack((im.transpose(1, 2, 0)[..., :3], target_as_rgb)))\n",
    "plt.title(sample_token)  # 设置图像标题为样本token\n",
    "plt.show()  # 显示图像\n",
    "\n",
    "# 使用样本token可视化对应的LiDAR数据\n",
    "visualize_lidar_of_sample(sample_token)\n"
   ],
   "id": "620e1ca0edb00961"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# U-Net网络\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 定义U-Net模型\n",
    "class UNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=1,        # 输入通道数，默认为1\n",
    "        n_classes=2,          # 输出通道数，默认为2（比如二分类）\n",
    "        depth=5,              # 网络的深度（卷积层的数量）\n",
    "        wf=6,                 # 第一个卷积层的滤波器数量是 2**wf\n",
    "        padding=False,        # 是否进行零填充\n",
    "        batch_norm=False,     # 是否在卷积层后使用批标准化\n",
    "        up_mode='upconv',     # 上采样方式，'upconv' 或 'upsample'\n",
    "    ):\n",
    "        \"\"\"\n",
    "        U-Net: Convolutional Networks for Biomedical Image Segmentation\n",
    "        使用默认参数时，得到的是原论文中使用的U-Net版本\n",
    "        \"\"\"\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # 检查up_mode是否有效\n",
    "        assert up_mode in ('upconv', 'upsample')\n",
    "        self.padding = padding\n",
    "        self.depth = depth\n",
    "        prev_channels = in_channels\n",
    "\n",
    "        # 下采样路径\n",
    "        self.down_path = nn.ModuleList()\n",
    "        for i in range(depth):\n",
    "            self.down_path.append(\n",
    "                UNetConvBlock(prev_channels, 2 ** (wf + i), padding, batch_norm)\n",
    "            )\n",
    "            prev_channels = 2 ** (wf + i)\n",
    "\n",
    "        # 上采样路径\n",
    "        self.up_path = nn.ModuleList()\n",
    "        for i in reversed(range(depth - 1)):\n",
    "            self.up_path.append(\n",
    "                UNetUpBlock(prev_channels, 2 ** (wf + i), up_mode, padding, batch_norm)\n",
    "            )\n",
    "            prev_channels = 2 ** (wf + i)\n",
    "\n",
    "        # 最后的1x1卷积层，输出预测结果\n",
    "        self.last = nn.Conv2d(prev_channels, n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        blocks = []\n",
    "        # 下采样过程\n",
    "        for i, down in enumerate(self.down_path):\n",
    "            x = down(x)\n",
    "            if i != len(self.down_path) - 1:\n",
    "                blocks.append(x)\n",
    "                x = F.max_pool2d(x, 2)  # 池化操作，下采样\n",
    "\n",
    "        # 上采样过程\n",
    "        for i, up in enumerate(self.up_path):\n",
    "            x = up(x, blocks[-i - 1])  # 跳跃连接，将下采样特征与上采样特征拼接\n",
    "\n",
    "        return self.last(x)  # 返回最终预测结果\n",
    "\n",
    "\n",
    "# 定义卷积块，包含两个卷积层\n",
    "class UNetConvBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, padding, batch_norm):\n",
    "        super(UNetConvBlock, self).__init__()\n",
    "        block = []\n",
    "\n",
    "        # 第一个卷积层\n",
    "        block.append(nn.Conv2d(in_size, out_size, kernel_size=3, padding=int(padding)))\n",
    "        block.append(nn.ReLU())\n",
    "        if batch_norm:\n",
    "            block.append(nn.BatchNorm2d(out_size))\n",
    "\n",
    "        # 第二个卷积层\n",
    "        block.append(nn.Conv2d(out_size, out_size, kernel_size=3, padding=int(padding)))\n",
    "        block.append(nn.ReLU())\n",
    "        if batch_norm:\n",
    "            block.append(nn.BatchNorm2d(out_size))\n",
    "\n",
    "        # 将卷积块连接起来\n",
    "        self.block = nn.Sequential(*block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)  # 执行卷积块的前向传播\n",
    "\n",
    "\n",
    "# 定义上采样块\n",
    "class UNetUpBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, up_mode, padding, batch_norm):\n",
    "        super(UNetUpBlock, self).__init__()\n",
    "\n",
    "        # 根据上采样模式选择不同的上采样方法\n",
    "        if up_mode == 'upconv':\n",
    "            self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=2, stride=2)\n",
    "        elif up_mode == 'upsample':\n",
    "            self.up = nn.Sequential(\n",
    "                nn.Upsample(mode='bilinear', scale_factor=2),  # 双线性插值上采样\n",
    "                nn.Conv2d(in_size, out_size, kernel_size=1),  # 1x1卷积\n",
    "            )\n",
    "\n",
    "        # 使用卷积块\n",
    "        self.conv_block = UNetConvBlock(in_size, out_size, padding, batch_norm)\n",
    "\n",
    "    def center_crop(self, layer, target_size):\n",
    "        \"\"\"\n",
    "        中心裁剪，用于对齐上采样和下采样路径的特征图\n",
    "        \"\"\"\n",
    "        _, _, layer_height, layer_width = layer.size()\n",
    "        diff_y = (layer_height - target_size[0]) // 2\n",
    "        diff_x = (layer_width - target_size[1]) // 2\n",
    "        return layer[\n",
    "            :, :, diff_y : (diff_y + target_size[0]), diff_x : (diff_x + target_size[1])\n",
    "        ]\n",
    "\n",
    "    def forward(self, x, bridge):\n",
    "        # 上采样\n",
    "        up = self.up(x)\n",
    "        # 中心裁剪\n",
    "        crop1 = self.center_crop(bridge, up.shape[2:])\n",
    "        # 拼接上采样结果与下采样结果\n",
    "        out = torch.cat([up, crop1], 1)\n",
    "        # 通过卷积块\n",
    "        out = self.conv_block(out)\n",
    "\n",
    "        return out\n"
   ],
   "id": "f37198270b493b09"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_unet_model(in_channels=3, num_output_classes=2):\n",
    "    # 创建一个 U-Net 模型\n",
    "    model = UNet(in_channels=in_channels,        # 输入通道数，默认为 3（RGB 图像）\n",
    "                 n_classes=num_output_classes,  # 输出类别数，默认为 2（适用于二分类任务）\n",
    "                 wf=5,                          # 网络的初始滤波器数\n",
    "                 depth=4,                       # 网络的深度（层数）\n",
    "                 padding=True,                  # 是否对输入进行零填充，使得输出尺寸和输入相同\n",
    "                 up_mode='upsample')            # 使用上采样进行上采样，采用双线性插值\n",
    "\n",
    "    # 可选：为了支持多 GPU 训练和推理，使用 DataParallel 进行封装\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "    # 返回训练好的 U-Net 模型\n",
    "    return model\n"
   ],
   "id": "6e3fccd0de51f6c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 可视化模型的输入图像、预测结果和目标标签\n",
    "def visualize_predictions(input_image, prediction, target, n_images=2, apply_softmax=True):\n",
    "    \"\"\"\n",
    "    输入三个 PyTorch 张量，绘制输入图像、预测结果和目标标签。\n",
    "    \"\"\"\n",
    "    # 只选择前 n 张图片\n",
    "    prediction = prediction[:n_images]\n",
    "    target = target[:n_images]\n",
    "    input_image = input_image[:n_images]\n",
    "\n",
    "    # 从计算图中分离张量并转移到 CPU，然后转换为 NumPy 数组\n",
    "    prediction = prediction.detach().cpu().numpy()\n",
    "    \n",
    "    # 如果需要，应用 softmax 将预测值转为概率分布\n",
    "    if apply_softmax:\n",
    "        prediction = scipy.special.softmax(prediction, axis=1)\n",
    "    \n",
    "    # 将第一类预测值进行反转 (1 - 预测值)\n",
    "    class_one_preds = np.hstack(1 - prediction[:, 0])\n",
    "\n",
    "    # 获取目标标签的 NumPy 数组\n",
    "    target = np.hstack(target.detach().cpu().numpy())\n",
    "\n",
    "    # 创建 RGB 颜色映射，红色通道为预测，绿色通道为目标标签\n",
    "    class_rgb = np.repeat(class_one_preds[..., None], 3, axis=2)\n",
    "    class_rgb[..., 2] = 0  # 将蓝色通道设置为 0\n",
    "    class_rgb[..., 1] = target  # 将绿色通道设置为目标标签值\n",
    "\n",
    "    # 将输入图像转为 NumPy 数组并进行转置\n",
    "    input_im = np.hstack(input_image.cpu().numpy().transpose(0, 2, 3, 1))\n",
    "\n",
    "    # 如果输入图像有 3 个通道（RGB），则转换为灰度图像\n",
    "    if input_im.shape[2] == 3:\n",
    "        input_im_grayscale = np.repeat(input_im.mean(axis=2)[..., None], 3, axis=2)\n",
    "        # 将灰度图像与预测结果叠加\n",
    "        overlayed_im = (input_im_grayscale * 0.6 + class_rgb * 0.7).clip(0, 1)\n",
    "    else:\n",
    "        # 如果图像有 4 个通道，则处理其中的 map 信息\n",
    "        input_map = input_im[..., 3:]\n",
    "        overlayed_im = (input_map * 0.6 + class_rgb * 0.7).clip(0, 1)\n",
    "\n",
    "    # 将预测值大于 0.5 的地方标记为真，生成二值图像\n",
    "    thresholded_pred = np.repeat(class_one_preds[..., None] > 0.5, 3, axis=2)\n",
    "\n",
    "    # 绘制图像\n",
    "    fig = plt.figure(figsize=(12, 26))\n",
    "    # 将图像按垂直方向堆叠，显示输入图像、预测图像、目标图像、叠加图像和二值化预测结果\n",
    "    plot_im = np.vstack([class_rgb, input_im[..., :3], overlayed_im, thresholded_pred]).clip(0, 1).astype(np.float32)\n",
    "    \n",
    "    # 显示图像\n",
    "    plt.imshow(plot_im)\n",
    "    plt.axis(\"off\")  # 不显示坐标轴\n",
    "    plt.show()\n"
   ],
   "id": "9fd202bcb81bdf8d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 我们通过降低类别 0 的损失权重来处理类别不平衡问题。\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # 选择计算设备，如果有 GPU 就用 GPU，否则用 CPU\n",
    "\n",
    "# 定义每个类别的权重：类别 0 的权重为 0.2，其余类别的权重为 1.0\n",
    "class_weights = torch.from_numpy(np.array([0.2] + [1.0]*len(classes), dtype=np.float32))\n",
    "\n",
    "# 将权重张量移动到选择的计算设备上\n",
    "class_weights = class_weights.to(device)\n"
   ],
   "id": "ec8e48fadcfe4eeb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "batch_size = 8\n",
    "epochs = 15  # 训练的轮数。为了缩短训练时间，这里设置为 15，实际训练中可以增加轮数来获得更好的结果。\n",
    "\n",
    "# 获取 U-Net 模型\n",
    "model = get_unet_model(num_output_classes=len(classes)+1)\n",
    "model = model.to(device)  # 将模型移到 GPU 或 CPU 上\n",
    "\n",
    "# 使用 Adam 优化器，学习率为 1e-3\n",
    "optim = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# 创建数据加载器\n",
    "dataloader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=True, num_workers=os.cpu_count()*2)\n",
    "\n",
    "all_losses = []  # 用来记录所有轮次的损失\n",
    "\n",
    "# 开始训练\n",
    "for epoch in range(1, epochs+1):\n",
    "    print(\"Epoch\", epoch)\n",
    "    \n",
    "    epoch_losses = []  # 每个 epoch 的损失\n",
    "    progress_bar = tqdm_notebook(dataloader)  # 显示进度条\n",
    "\n",
    "    for ii, (X, target, sample_ids) in enumerate(progress_bar):\n",
    "        X = X.to(device)  # 输入图像，大小为 [N, 3, H, W]\n",
    "        target = target.to(device)  # 目标标签，大小为 [N, H, W]\n",
    "        prediction = model(X)  # 模型预测结果，大小为 [N, 2, H, W]\n",
    "        \n",
    "        # 计算损失，使用加权交叉熵\n",
    "        loss = F.cross_entropy(prediction, target, weight=class_weights)\n",
    "\n",
    "        # 反向传播\n",
    "        optim.zero_grad()  # 清空梯度\n",
    "        loss.backward()  # 反向传播\n",
    "        optim.step()  # 更新参数\n",
    "        \n",
    "        # 记录损失\n",
    "        epoch_losses.append(loss.detach().cpu().numpy())\n",
    "\n",
    "        # 每个 epoch 的第一个批次可视化预测结果\n",
    "        if ii == 0:\n",
    "            visualize_predictions(X, prediction, target)\n",
    "    \n",
    "    print(\"Loss:\", np.mean(epoch_losses))  # 打印当前 epoch 的平均损失\n",
    "    all_losses.extend(epoch_losses)  # 将当前 epoch 的损失添加到总损失列表\n",
    "    \n",
    "    # 保存模型的检查点\n",
    "    checkpoint_filename = \"unet_checkpoint_epoch_{}.pth\".format(epoch)\n",
    "    checkpoint_filepath = os.path.join(ARTIFACTS_FOLDER, checkpoint_filename)\n",
    "    torch.save(model.state_dict(), checkpoint_filepath)\n",
    "    \n",
    "# 绘制训练过程中损失的变化曲线\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.plot(all_losses, alpha=0.75)\n",
    "plt.show()\n"
   ],
   "id": "3d9f4c09f87f001b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 获取验证数据集的输入图像和目标标签路径，并进行排序\n",
    "input_filepaths = sorted(glob.glob(os.path.join(validation_data_folder, \"*_input.png\")))\n",
    "target_filepaths = sorted(glob.glob(os.path.join(validation_data_folder, \"*_target.png\")))\n",
    "\n",
    "# 设置批次大小为16\n",
    "batch_size = 16\n",
    "\n",
    "# 创建验证数据集对象\n",
    "validation_dataset = BEVImageDataset(input_filepaths, target_filepaths)\n",
    "\n",
    "# 创建数据加载器，设置批次大小，并禁用随机打乱数据\n",
    "validation_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size, shuffle=False, num_workers=os.cpu_count())\n"
   ],
   "id": "6417a2485f6b19a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 验证集推理，计算损失值\n",
    "# 使用tqdm_notebook展示训练进度条\n",
    "progress_bar = tqdm_notebook(validation_dataloader)\n",
    "\n",
    "# 初始化目标和预测数组，用于存储所有的目标标签和预测结果\n",
    "# 使用uint8类型以节省内存，否则会占用超过20GB的内存\n",
    "targets = np.zeros((len(target_filepaths), 336, 336), dtype=np.uint8)\n",
    "predictions = np.zeros((len(target_filepaths), 1 + len(classes), 336, 336), dtype=np.uint8)\n",
    "\n",
    "# 用于存储样本token和损失值的列表\n",
    "sample_tokens = []\n",
    "all_losses = []\n",
    "\n",
    "# 不需要计算梯度，因此使用`torch.no_grad()`来节省内存\n",
    "with torch.no_grad():\n",
    "    # 设置模型为评估模式\n",
    "    model.eval()\n",
    "    # 遍历验证数据集\n",
    "    for ii, (X, target, batch_sample_tokens) in enumerate(progress_bar):\n",
    "\n",
    "        # 根据当前批次更新目标数组\n",
    "        offset = ii * batch_size\n",
    "        targets[offset:offset + batch_size] = target.numpy()\n",
    "        sample_tokens.extend(batch_sample_tokens)\n",
    "        \n",
    "        # 将输入和目标数据转移到设备（GPU或CPU）\n",
    "        X = X.to(device)  # 输入的形状为 [N, 1, H, W]\n",
    "        target = target.to(device)  # 目标的形状为 [N, H, W]，包含类别索引（0, 1）\n",
    "        \n",
    "        # 获取模型预测的输出，形状为 [N, 2, H, W]（2表示类别数）\n",
    "        prediction = model(X)  # [N, 2, H, W]\n",
    "        \n",
    "        # 计算损失值，这里使用了加权交叉熵损失函数\n",
    "        loss = F.cross_entropy(prediction, target, weight=class_weights)\n",
    "        all_losses.append(loss.detach().cpu().numpy())\n",
    "        \n",
    "        # 对预测结果进行softmax操作\n",
    "        prediction = F.softmax(prediction, dim=1)\n",
    "        \n",
    "        # 将预测结果转回CPU并转换为numpy格式\n",
    "        prediction_cpu = prediction.cpu().numpy()\n",
    "        \n",
    "        # 将预测结果量化为[0, 255]的uint8类型\n",
    "        predictions[offset:offset + batch_size] = np.round(prediction_cpu * 255).astype(np.uint8)\n",
    "        \n",
    "        # 可视化第一次的预测结果\n",
    "        if ii == 0:\n",
    "            visualize_predictions(X, prediction, target, apply_softmax=False)\n",
    "            \n",
    "# 输出平均损失\n",
    "print(\"Mean loss:\", np.mean(all_losses))\n"
   ],
   "id": "3eb8ff95054d4b15"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 预测结果和目标数据进行可视化展示\n",
    "predictions_non_class0 = 255 - predictions[:, 0]\n",
    "\n",
    "# 设置背景的阈值，这里背景阈值为 127\n",
    "background_threshold = 255 // 2\n",
    "\n",
    "# 遍历前 3 张图片进行展示\n",
    "for i in range(3):\n",
    "    # 创建一个 1 行 3 列的子图，图像尺寸为 (16, 6)\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(16, 6))\n",
    "    \n",
    "    # 绘制非背景类的预测图\n",
    "    axes[0].imshow(predictions_non_class0[i])\n",
    "    axes[0].set_title(\"predictions\")  # 设置标题为“predictions”\n",
    "    \n",
    "    # 绘制经过阈值化后的预测图，预测大于阈值的像素为前景，其他为背景\n",
    "    axes[1].imshow(predictions_non_class0[i] > background_threshold)\n",
    "    axes[1].set_title(\"thresholded predictions\")  # 设置标题为“thresholded predictions”\n",
    "    \n",
    "    # 绘制目标图像，背景为 0，前景为 1\n",
    "    axes[2].imshow((targets[i] > 0).astype(np.uint8), interpolation=\"nearest\")\n",
    "    axes[2].set_title(\"targets\")  # 设置标题为“targets”\n",
    "    \n",
    "    # 调整子图之间的间距，使得显示更紧凑\n",
    "    fig.tight_layout()\n",
    "    fig.show()  # 显示图像\n"
   ],
   "id": "868980282b2c4a3e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 使用形态学开运算进行过滤\n",
    "# 使用椭圆形内核创建结构元素进行形态学操作\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "\n",
    "# 初始化一个与预测图像相同尺寸的空数组，用于保存形态学开运算后的结果\n",
    "predictions_opened = np.zeros((predictions_non_class0.shape), dtype=np.uint8)\n",
    "\n",
    "# 遍历所有的预测图像\n",
    "for i, p in enumerate(tqdm(predictions_non_class0)):\n",
    "    # 将预测值大于背景阈值的位置设置为1，其他为0（进行二值化处理）\n",
    "    thresholded_p = (p > background_threshold).astype(np.uint8)\n",
    "    \n",
    "    # 对二值化后的图像执行开运算，去除小的噪声区域\n",
    "    predictions_opened[i] = cv2.morphologyEx(thresholded_p, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "# 绘制原始二值化图像和开运算后的图像对比\n",
    "plt.figure(figsize=(12, 12))\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 6))\n",
    "\n",
    "# 显示阈值化后的预测图像\n",
    "axes[0].imshow(predictions_non_class0[0] > 255 // 2)\n",
    "axes[0].set_title(\"thresholded prediction\")\n",
    "\n",
    "# 显示开运算后的预测图像\n",
    "axes[1].imshow(predictions_opened[0])\n",
    "axes[1].set_title(\"opened thresholded prediction\")\n",
    "\n",
    "# 显示图像\n",
    "fig.show()\n"
   ],
   "id": "f0cf7fb2ea35f648"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 使用 scipy.ndimage.label 来标记预测图像中的连通组件\n",
    "labels, n = scipy.ndimage.label(predictions_opened[0])\n",
    "\n",
    "# 绘制标记后的连通组件图像\n",
    "plt.imshow(labels, cmap=\"tab20b\")\n",
    "\n",
    "# 在图像中添加标签，显示连通组件的数量\n",
    "plt.xlabel(\"N predictions: {}\".format(n))\n",
    "\n",
    "# 显示图像\n",
    "plt.show()\n"
   ],
   "id": "a4ca2909405f2aa3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 提取检测框和计算得分\n",
    "detection_boxes = []\n",
    "detection_scores = []\n",
    "detection_classes = []\n",
    "\n",
    "for i in tqdm_notebook(range(len(predictions))):\n",
    "    prediction_opened = predictions_opened[i]\n",
    "    probability_non_class0 = predictions_non_class0[i]\n",
    "    class_probability = predictions[i]\n",
    "\n",
    "    sample_boxes = []\n",
    "    sample_detection_scores = []\n",
    "    sample_detection_classes = []\n",
    "    \n",
    "    contours, hierarchy = cv2.findContours(prediction_opened, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE) \n",
    "    \n",
    "    for cnt in contours:\n",
    "        rect = cv2.minAreaRect(cnt)\n",
    "        box = cv2.boxPoints(rect)\n",
    "        \n",
    "        # 根据中心像素得到置信度\n",
    "        box_center_index = np.int0(np.mean(box, axis=0))\n",
    "        \n",
    "        for class_index in range(len(classes)):\n",
    "            box_center_value = class_probability[class_index+1, box_center_index[1], box_center_index[0]]\n",
    "            \n",
    "            if box_center_value < 0.01:\n",
    "                continue\n",
    "            \n",
    "            box_center_class = classes[class_index]\n",
    "\n",
    "            box_detection_score = box_center_value\n",
    "            sample_detection_classes.append(box_center_class)\n",
    "            sample_detection_scores.append(box_detection_score)\n",
    "            sample_boxes.append(box)\n",
    "        \n",
    "    \n",
    "    detection_boxes.append(np.array(sample_boxes))\n",
    "    detection_scores.append(sample_detection_scores)\n",
    "    detection_classes.append(sample_detection_classes)\n",
    "    \n",
    "print(\"Total amount of boxes:\", np.sum([len(x) for x in detection_boxes]))\n",
    "    \n",
    "\n",
    "# Visualize the boxes in the first sample\n",
    "t = np.zeros_like(predictions_opened[0])\n",
    "for sample_boxes in detection_boxes[0]:\n",
    "    box_pix = np.int0(sample_boxes)\n",
    "    cv2.drawContours(t,[box_pix],0,(255),2)\n",
    "plt.imshow(t)\n",
    "plt.show()\n",
    "\n",
    "# Visualize their probabilities\n",
    "plt.hist(detection_scores[0], bins=20)\n",
    "plt.xlabel(\"Detection Score\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ],
   "id": "4ff8ffcf59c38636"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 加载真实值\n",
    "from lyft_dataset_sdk.eval.detection.mAP_evaluation import Box3D, recall_precision\n",
    "\n",
    "def load_groundtruth_boxes(nuscenes, sample_tokens):\n",
    "    gt_box3ds = []\n",
    "\n",
    "    # Load annotations and filter predictions and annotations.\n",
    "    for sample_token in tqdm_notebook(sample_tokens):\n",
    "\n",
    "        sample = nuscenes.get('sample', sample_token)\n",
    "        sample_annotation_tokens = sample['anns']\n",
    "\n",
    "        sample_lidar_token = sample[\"data\"][\"LIDAR_TOP\"]\n",
    "        lidar_data = level5data.get(\"sample_data\", sample_lidar_token)\n",
    "        ego_pose = level5data.get(\"ego_pose\", lidar_data[\"ego_pose_token\"])\n",
    "        ego_translation = np.array(ego_pose['translation'])\n",
    "        \n",
    "        for sample_annotation_token in sample_annotation_tokens:\n",
    "            sample_annotation = nuscenes.get('sample_annotation', sample_annotation_token)\n",
    "            sample_annotation_translation = sample_annotation['translation']\n",
    "            \n",
    "            class_name = sample_annotation['category_name']\n",
    "            \n",
    "            box3d = Box3D(\n",
    "                sample_token=sample_token,\n",
    "                translation=sample_annotation_translation,\n",
    "                size=sample_annotation['size'],\n",
    "                rotation=sample_annotation['rotation'],\n",
    "                name=class_name\n",
    "            )\n",
    "            gt_box3ds.append(box3d)\n",
    "            \n",
    "    return gt_box3ds\n",
    "\n",
    "gt_box3ds = load_groundtruth_boxes(level5data, sample_tokens)"
   ],
   "id": "3b02f69bc546c5b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 初始化存储 3D 预测框的列表\n",
    "pred_box3ds = []\n",
    "\n",
    "# 遍历每个样本的检测框\n",
    "for (sample_token, sample_boxes, sample_detection_scores, sample_detection_class) in tqdm_notebook(zip(sample_tokens, detection_boxes, detection_scores, detection_classes), total=len(sample_tokens)):\n",
    "    \n",
    "    # 将每个检测框的数据重新调整形状\n",
    "    sample_boxes = sample_boxes.reshape(-1, 2)  # (N, 4, 2) -> (N*4, 2)\n",
    "    sample_boxes = sample_boxes.transpose(1,0)  # (N*4, 2) -> (2, N*4)\n",
    "    \n",
    "    # 为检测框添加 Z 维度（默认为 0，即假设目标的 Z 坐标为 0）\n",
    "    sample_boxes = np.vstack((sample_boxes, np.zeros(sample_boxes.shape[1]),))  # (2, N*4) -> (3, N*4)\n",
    "    \n",
    "    # 获取当前样本的相关信息\n",
    "    sample = level5data.get(\"sample\", sample_token)\n",
    "    sample_lidar_token = sample[\"data\"][\"LIDAR_TOP\"]  # 获取 LIDAR 数据的标识符\n",
    "    lidar_data = level5data.get(\"sample_data\", sample_lidar_token)  # 获取 LIDAR 数据\n",
    "    lidar_filepath = level5data.get_sample_data_path(sample_lidar_token)  # 获取 LIDAR 数据文件路径\n",
    "    ego_pose = level5data.get(\"ego_pose\", lidar_data[\"ego_pose_token\"])  # 获取 Ego 车的姿态信息\n",
    "    ego_translation = np.array(ego_pose['translation'])  # Ego 车的位置\n",
    "    \n",
    "    # 计算从 Ego 车坐标系到全球坐标系的转换矩阵\n",
    "    global_from_car = transform_matrix(ego_pose['translation'], Quaternion(ego_pose['rotation']), inverse=False)\n",
    "    \n",
    "    # 计算从车载坐标系到体素坐标系的转换矩阵，并得到全局到体素的转换矩阵\n",
    "    car_from_voxel = np.linalg.inv(create_transformation_matrix_to_voxel_space(bev_shape, voxel_size, (0, 0, z_offset)))\n",
    "    global_from_voxel = np.dot(global_from_car, car_from_voxel)\n",
    "    \n",
    "    # 将检测框从车载坐标系转换到体素坐标系\n",
    "    sample_boxes = transform_points(sample_boxes, global_from_voxel)\n",
    "\n",
    "    # 假设所有的目标框都与 Ego 车的 Z 坐标相同，因此将 Z 轴的值设为 Ego 车的 Z 坐标\n",
    "    sample_boxes[2,:] = ego_pose[\"translation\"][2]\n",
    "    \n",
    "    # 调整框的维度：从 (3, N*4) 转换为 (N, 4, 3)，每个检测框由 4 个点的 3D 坐标组成\n",
    "    sample_boxes = sample_boxes.transpose(1,0).reshape(-1, 4, 3)\n",
    "    \n",
    "    # 假设所有目标的高度为 1.75\n",
    "    box_height = 1.75\n",
    "    \n",
    "    # 计算每个框的中心位置（X, Y, Z）\n",
    "    sample_boxes_centers = sample_boxes.mean(axis=1)\n",
    "    sample_boxes_centers[:,2] += box_height/2  # 假设目标框底部与地面接触，因此需要加上目标高度的一半\n",
    "    \n",
    "    # 计算每个框的长度和宽度，使用欧几里得距离计算两点之间的距离\n",
    "    sample_lengths = np.linalg.norm(sample_boxes[:,0,:] - sample_boxes[:,1,:], axis=1) * 1/box_scale\n",
    "    sample_widths = np.linalg.norm(sample_boxes[:,1,:] - sample_boxes[:,2,:], axis=1) * 1/box_scale\n",
    "    \n",
    "    # 构建每个框的尺寸信息（宽度、长度、高度）\n",
    "    sample_boxes_dimensions = np.zeros_like(sample_boxes_centers) \n",
    "    sample_boxes_dimensions[:,0] = sample_widths\n",
    "    sample_boxes_dimensions[:,1] = sample_lengths\n",
    "    sample_boxes_dimensions[:,2] = box_height\n",
    "\n",
    "    # 遍历每个框，计算它们的旋转信息并将其转换为四元数\n",
    "    for i in range(len(sample_boxes)):\n",
    "        translation = sample_boxes_centers[i]  # 目标框的中心位置\n",
    "        size = sample_boxes_dimensions[i]  # 目标框的尺寸（宽度、长度、高度）\n",
    "        class_name = sample_detection_class[i]  # 目标类别\n",
    "        ego_distance = float(np.linalg.norm(ego_translation - translation))  # 计算目标与 Ego 车的距离\n",
    "        \n",
    "        # 计算框的旋转信息，假设框的旋转矩阵可以通过框的两个角点计算\n",
    "        v = (sample_boxes[i,0] - sample_boxes[i,1])\n",
    "        v /= np.linalg.norm(v)  # 将向量标准化\n",
    "        r = R.from_dcm([  # 使用旋转矩阵创建四元数\n",
    "            [v[0], -v[1], 0],\n",
    "            [v[1],  v[0], 0],\n",
    "            [   0,     0, 1],\n",
    "        ])\n",
    "        quat = r.as_quat()  # 转换为四元数\n",
    "        quat = quat[[3,0,1,2]]  # 将四元数顺序调整为 XYZW -> WXYZ\n",
    "        \n",
    "        detection_score = float(sample_detection_scores[i])  # 获取检测得分\n",
    "        \n",
    "        # 创建 Box3D 对象，表示一个 3D 预测框\n",
    "        box3d = Box3D(\n",
    "            sample_token=sample_token,  # 样本的 token\n",
    "            translation=list(translation),  # 目标框的中心位置\n",
    "            size=list(size),  # 目标框的尺寸\n",
    "            rotation=list(quat),  # 目标框的旋转四元数\n",
    "            name=class_name,  # 目标类别\n",
    "            score=detection_score  # 检测得分\n",
    "        )\n",
    "        \n",
    "        # 将创建的 3D 预测框添加到列表中\n",
    "        pred_box3ds.append(box3d)"
   ],
   "id": "d91400b8541d1020"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
